{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos para Estudo de Generalização do Blend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse notebook é responsável pela análise de generalização das bases do Blend. A generalização é analisada à luz da análise de componentes principais, da estimação da função densidade de probabilidade (*pdf*, do inglês *Probability Density Function*) e da análise de agrupamentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import e Definições"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse conjunto de células deve ser sempre executado, uma vez que todas as outras células vão necessitar de bibliotecas e variáveis aqui definidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn.externals import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "%matplotlib inline\n",
    "import os\n",
    "from sklearn import decomposition\n",
    "\n",
    "import blend.plots as blend_plots\n",
    "\n",
    "from sklearn import manifold\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "import matplotlib as mpl\n",
    "from scipy.stats import gaussian_kde\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "\n",
    "clus_colors = ['#000000', '#ff5555','#55ff55','#5555ff',\n",
    "               '#55ffff','#ffff55','#4F3101','#EB9AEC',\n",
    "               '#0000ff','#999999','#ffbb44']\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importação dos dados de desenvolvimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os cógidos consideram que os dados estão formatados segundo um **DataFrame**, definido na biblioteca **pandas**. As colunas desse **DataFrame** são os nomes das propriedades, enquanto o índice deve ser o identificador da amostra do petróleo. No exemplo abaixo, os dados da base pública de 49 petróleos são importados e filtrados para a utilização dos valores referentes ao corte *CRD*, ou cru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cutname = 'CRD'\n",
    "blend_data_dir = os.getenv('BLENDDATA')\n",
    "fname = blend_data_dir + '/blend_dev.jbl'\n",
    "obj = joblib.load(fname)\n",
    "raw_data = obj[cutname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importação dos dados do Blend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os cógidos consideram que os dados estão formatados segundo um **DataFrame**, definido na biblioteca **pandas**. As colunas desse **DataFrame** são os nomes das propriedades, enquanto o índice deve ser o identificador da amostra do petróleo. No caso dos dados da base homologada do Blend, eles já estão filtrados considerando as propriedades globais do petróleo. O mapa de propriedades também é importado e adicionado aos dados da base para possibilitar a comparação com os dados de desenvolvimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filedata = blend_data_dir + '/blend_crude_data.pck'\n",
    "raw_apply_data = joblib.load(filedata) # pandas DataFrame\n",
    "prop_map = joblib.load(blend_data_dir + '/property_map.pck')\n",
    "raw_apply_data.columns = [prop_map[k] for k in raw_apply_data.columns]\n",
    "raw_apply_data = raw_apply_data[raw_data.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleção de Propriedades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa célula possibilita a escolha de quais propriedades devem ser consideradas para a análise. Ela também define alguns tipos de propriedades, como aquelas referentes à viscosidade do petróleo. Caso não deva haver seleção de propriedades (utilização de tudo que tem nos dados importados), a variável *propnames* deve ter seu valor alterado para *None*.\n",
    "\n",
    "A variável *custom_prop* pode ser utilizada para a criação de propriedades customizadas. Para criar nada, essa variável deve ter o seu valor alterado para *None*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# empty list means all properties in the input dataframe\n",
    "prop_list = ['INIK','IVAN','Metais','PAPC','PV20',\n",
    "             'PV30','PCCS','IACD','FCFM']\n",
    "prop_list = ['IACD','ITNT','IBNT','IPOR','ICON','INIK','IVAN',\n",
    "             'PAPC','PCCS','IASP','P010','P030','P050','P070',\n",
    "             'P090','IFAR','IFST','PV20','PV30','PV40','PV50']\n",
    "viscosity_props = ['PV20','PV30','PV40','PV50']\n",
    "# Custom properties, if any\n",
    "custom_prop = None\n",
    "if custom_prop is not None:\n",
    "    if custom_prop.lower() == 'metais':\n",
    "        raw_data[custom_prop] = raw_data['INIK'] + raw_data['IVAN']\n",
    "    else:\n",
    "        raise Exception(u'Propriedade desconhecida: ' + custom_prop)\n",
    "# Filter\n",
    "if len(prop_list) != 0:\n",
    "    raw_data = raw_data[prop_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa célula é responsável pela normalização das propriedades consideradas na análise. Normalizações dedicadas para cada propriedade e um método global (normalização padrão) são executados. A variável *raw_data* é mantida apenas com o valor das propriedades considerando as normalizações dedicadas, sem a normalização padrão.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PROPERTY DEPENDENT\n",
    "# VISCOSITY\n",
    "for prop in viscosity_props:\n",
    "    raw_data[prop] = np.log(raw_data[prop])    \n",
    "    raw_apply_data[prop] = np.log(raw_apply_data[prop])    \n",
    "# PCCS\n",
    "if 'PCCS' in raw_data.columns:\n",
    "    if raw_data['PCCS'].max() > 10:\n",
    "        raw_data['PCCS'] = raw_data['PCCS'] / 1000.0\n",
    "    \n",
    "# STANDARD NORMALIZATION\n",
    "import sklearn.preprocessing as preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "#scaler = preprocessing.MinMaxScaler()\n",
    "data = pandas.DataFrame(scaler.fit_transform(raw_data),\n",
    "                        columns=raw_data.columns, index=raw_data.index)\n",
    "apply_data = pandas.DataFrame(scaler.fit_transform(raw_apply_data),\n",
    "                              columns=raw_apply_data.columns,\n",
    "                              index=raw_apply_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribuições"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa seção executa a análise de generalização considerando a estimativa das distribuições *pdf* das propriedades das duas bases de dados. A variável *nsamples* controla a quantidade de distribuições aleatórias, considerando a *pdf* estimada da base de dados de desenvolvimento, e a quantidade de sorteios para cada distribuição. As distribuições aleatórias são estimadas *ninit* vezes de forma a considerar a flutuação dos geradores aleatórios. Essas distribuições aleatórias são utilizadas para calibrar a divergência de Kullback-Leibler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_from_dist(xmodel, ymodel, ndata, fPlot=False, fMethod = True):\n",
    "    if fMethod:\n",
    "        bin_midpoints = xmodel + (xmodel[1] - xmodel[0])/2\n",
    "        cdf = np.cumsum(ymodel)\n",
    "        cdf = cdf / cdf[-1]\n",
    "        values = np.random.rand(ndata)\n",
    "        value_bins = np.searchsorted(cdf, values)\n",
    "        random_from_cdf = bin_midpoints[value_bins]\n",
    "    else:\n",
    "        random_from_cdf = np.random.choice(xmodel, ndata, p = ymodel/ymodel.sum())\n",
    "    if fPlot:\n",
    "        a = plt.hist(random_from_cdf, bins=xmodel, normed=True)\n",
    "        plt.plot(xmodel, ymodel * a[0].sum() / ymodel.sum())\n",
    "    return random_from_cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop over Properties\n",
    "nsamples = [100, 500, 1000]\n",
    "ninit = 100\n",
    "Nx = 4\n",
    "Ny = int(raw_data.shape[1]/float(Nx)) + 1\n",
    "plt.figure(figsize=(6*Nx, 5*Ny))\n",
    "for iprop, prop in enumerate(raw_data.columns):\n",
    "    plt.subplot(Ny, Nx, iprop+1)\n",
    "    print prop, ', ',\n",
    "    test_data = raw_data[prop].values\n",
    "    #test_data = test_data[test_data < 5000]\n",
    "    X = np.linspace(test_data.min(),test_data.max(), 100)\n",
    "    ymodel = stats.gaussian_kde(test_data)(X)\n",
    "    # Plot basic PDF from KDE estimation\n",
    "    plt.plot(X, ymodel, lw=3, color='k', label='Modelo')\n",
    "    # Generate samples\n",
    "    for n in nsamples:\n",
    "        yapply = np.zeros((ninit, X.shape[0]))\n",
    "        kls = np.zeros(ninit)\n",
    "        for iinit in range(ninit):\n",
    "            ds_hist = random_from_dist(X, ymodel, n, False, False)\n",
    "            yapply[iinit] = stats.gaussian_kde(ds_hist)(X)\n",
    "            kls[iinit] = stats.entropy(ymodel, yapply[iinit])\n",
    "        plt.errorbar(X, np.nanmean(yapply,axis=0),np.nanstd(yapply,axis=0),\n",
    "                     errorevery=10,lw=3\n",
    "                     label='#R %i: %.3f +- %.3f'%(n, np.mean(kls), np.std(kls)))\n",
    "    # PLOT THE APPLY DATA\n",
    "    test_data = raw_apply_data[prop].values\n",
    "    X = np.linspace(test_data.min(),test_data.max(), 100)\n",
    "    yapply = stats.gaussian_kde(test_data)(X)\n",
    "    # Plot basic PDF from KDE estimation\n",
    "    kl = stats.entropy(ymodel, yapply)\n",
    "    plt.plot(X, yapply,'--k', lw=3, label='Blend: %.3f'%(kl))\n",
    "    plt.title(prop)\n",
    "    plt.legend(loc='best')\n",
    "#plt.suptitle('KL - KDE Gaussian', fontsize=14)\n",
    "print ' done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogramas das propriedades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for iprop, prop in enumerate(raw_data.columns.values):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.hist(raw_data[prop].values, 20)\n",
    "    plt.title('Modelo')\n",
    "    plt.ylabel(prop)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(raw_apply_data[prop].values, 20)\n",
    "    plt.title('Blend')\n",
    "    plt.ylabel(prop)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Análise de Componentes Principais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa seção, a generalização é estudada considerando a análise de componentes principais. As duas bases são comparadas considerando a curva de carga do PCA, quanto à contribuição das propriedades para a composição dos componentes principais e quanto ao ângulo entre os componentes principais extraídos considerando as duas bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pcaModel = decomposition.PCA().fit(data)\n",
    "pcaBlend = decomposition.PCA().fit(apply_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curva de Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Charge Curve - Modelo\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.cumsum(pcaModel.explained_variance_ratio_),'o-', \n",
    "         w=3, label='Modelo')\n",
    "plt.plot(np.cumsum(pcaBlend.explained_variance_ratio_),'^--',\n",
    "         lw=3, label='Blend')\n",
    "plt.plot([0, data.shape[1]],[0.9,0.9],'k--')\n",
    "plt.xlim([0, data.shape[1] - 0.9])\n",
    "plt.ylim([plt.axis()[2], 1.001])\n",
    "plt.ylabel('Taxa de Energia Acumulada')\n",
    "plt.xlabel('# Componentes Acumulados')\n",
    "plt.title('Curva de Carga')\n",
    "plt.legend(loc='center right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direções Principais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = [0,1,14,18]\n",
    "Nx = 2\n",
    "Ny = len(components)\n",
    "plt.figure(figsize=(8*Nx, 4*Ny))\n",
    "\n",
    "#for ipca in range(pcaModel.components_.shape[0]):\n",
    "    #plt.figure(figsize=(10,2))\n",
    "for pcanum, ipca in enumerate(components):\n",
    "    \n",
    "    \n",
    "    total_model = np.sum(np.abs(pcaModel.components_[ipca, :]))\n",
    "    total_blend = np.sum(np.abs(pcaBlend.components_[ipca, :]))    \n",
    "    val_model = 100 * np.abs(pcaModel.components_[ipca, :])/total_model\n",
    "    val_blend = 100 * np.abs(pcaBlend.components_[ipca, :])/total_blend\n",
    "    \n",
    "    plt.subplot(Ny, 2, (pcanum*2)+1)\n",
    "    #plt.subplot(1,2,1)\n",
    "    plt.bar(np.arange(val_model.shape[0]), val_model)    \n",
    "    plt.ylabel('PCA %i - Modelo'%(ipca+1))\n",
    "    plt.gca().set_xticks(np.arange(data.columns.shape[0])+0.5)\n",
    "    plt.gca().set_xticklabels(data.columns, rotation='90')\n",
    "    plt.xlim([0, data.columns.shape[0]])\n",
    "\n",
    "    plt.subplot(Ny, 2, (pcanum*2)+2)    \n",
    "    #plt.subplot(1,2,2)\n",
    "    plt.ylabel('PCA %i - Blend'%(ipca+1))    \n",
    "    plt.bar(np.arange(val_model.shape[0]), val_blend)\n",
    "    plt.gca().set_xticks(np.arange(data.columns.shape[0])+0.5)\n",
    "    plt.gca().set_xticklabels(data.columns, rotation='90')\n",
    "    plt.xlim([0, data.columns.shape[0]])    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ângulo entre as direções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "angles = np.zeros((pcaModel.components_.shape[0],\n",
    "                   pcaModel.components_.shape[0]))\n",
    "for i in range(pcaModel.components_.shape[0]):\n",
    "    for j in range(i, pcaModel.components_.shape[0]):\n",
    "        v1 = pcaModel.components_[i]/np.linalg.norm(pcaModel.components_[i],2)\n",
    "        v2 = pcaBlend.components_[j]/np.linalg.norm(pcaBlend.components_[j],2)\n",
    "        angles[i,j] = np.arccos(v1.dot(v2)) * 180 / np.pi\n",
    "angles[np.isnan(angles)] = 0\n",
    "idx = (angles > 90)&(angles <= 180)\n",
    "angles[idx] = angles[idx] - 180\n",
    "idx = (angles > 180)&(angles <= 270)\n",
    "angles[idx]= angles[idx] - 180\n",
    "angles = np.abs(angles)\n",
    "angles[np.tril_indices(angles.shape[0],-1)] = np.nan\n",
    "plt.matshow(angles, cmap=plt.cm.jet, vmin=0, vmax=90, fignum=0)\n",
    "plt.ylabel('PCA - Blend')\n",
    "plt.xlabel('PCA - Modelo')\n",
    "plt.gca().set_xticklabels(np.array(plt.gca().get_xticks()+1, 'i'))\n",
    "plt.gca().set_yticklabels(np.array(plt.gca().get_yticks()+1, 'i'))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agrupamentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa seção, a generalização é estudada considerando o algoritmo de agrupamentos *k-médias*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas propriedades podem ser removidas da análise de agrupamentos através da variável *black_list*. Se nenhuma propriedade deve ser removida, essa variável deve ter seu valor alterado para *None*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "black_list = ['PCCS', 'PV20', 'PV30', 'PV40', 'PV50']\n",
    "if black_list is not None:\n",
    "    cluster_data = data.drop(black_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Médias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo é executado *ninit* vezes, de forma a avaliar o impacto de sua inicialização. Por fim, será gerada a figura do *errorbar* considerando o valor médio e o desvio-padrão do índice de silhueta para as *ninit* inicializações. Adicionalmente, é mostrada a figura com o índice de silhueta individual para cada uma das configurações na variável *test_clusters*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ninit = 50\n",
    "nclusters = np.arange(2,11)\n",
    "test_clusters = [3]\n",
    "km_models = {}\n",
    "si_values = np.zeros((len(nclusters), ninit))\n",
    "for i, n in enumerate(nclusters):\n",
    "    perf = 0\n",
    "    for j in range(nInit):\n",
    "        km = cluster.KMeans(n, n_init = 1, init='k-means++')\n",
    "        km.fit(cluster_data)\n",
    "        si_values[i,j] = metrics.silhouette_score(cluster_data, km.labels_,\n",
    "                                                  metric='euclidean')\n",
    "        if perf < si_values[i,j]:\n",
    "            perf = si_values[i,j]\n",
    "            km_models[n] = km\n",
    "# Sillhouete\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.errorbar(nclusters, np.mean(si_values, axis=1),\n",
    "             np.std(si_values, axis=1), lw=2)\n",
    "plt.xlim([nclusters[0]-0.5, nclusters[-1]+0.5])\n",
    "plt.xlabel('# Clusters')\n",
    "plt.ylabel(u'Silhueta Média')\n",
    "plt.title(u'Avaliação de Clusters')\n",
    "plt.grid(True)\n",
    "\n",
    "# Individual sillhouete\n",
    "plt.figure(figsize=(8, 4*len(test_clusters)))\n",
    "for idx, nclus in enumerate(test_clusters):\n",
    "    plt.subplot(1,len(test_clusters),idx+1)\n",
    "    s = metrics.silhouette_samples(cluster_data, km_models[nclus].labels_)\n",
    "    blend_plots.plot_silhouette(s,km_models[nclus].labels_\n",
    "                                ,clus_colors,ax=plt.gca())\n",
    "    plt.xlabel('Silhueta')\n",
    "    plt.ylabel('Cluster')\n",
    "    plt.title('')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicação do Modelo no Blend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa subseção, o modelo de agrupamentos encontrado para a base de desenvolvimento é aplicada à base de homologação do Blend. O índice de silhueta individual é mostrado, considerando as amostras da base de homologação do Blend, de forma a avaliar se essas amostras parecem bem condicionadas aos seus agrupamentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_apply_data = apply_data[cluster_data.columns]\n",
    "plt.figure(figsize=(8, 4*len(test_clusters)))\n",
    "for idx, nclus in enumerate(test_clusters):\n",
    "    plt.subplot(1,len(test_clusters),idx+1)\n",
    "    pred_clusters = km_models[nclus].predict(cluster_apply_data)\n",
    "    s = metrics.silhouette_samples(cluster_apply_data, pred_clusters)\n",
    "    blend_plots.plot_silhouette(s,km_models[nclus].labels_,clus_colors,\n",
    "                                ax=plt.gca())\n",
    "    plt.xlabel('Silhueta')\n",
    "    plt.ylabel('Cluster')\n",
    "    plt.title('')\n",
    "    plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
